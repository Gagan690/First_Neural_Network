{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# SSL : Ladder Network\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader,Subset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "KP5o0Pvw9vwG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ladder Network"
      ],
      "metadata": {
        "id": "66BuAoKZB2dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# create loader : 100 examples across all the classes\n",
        "\n",
        "num_labels = 100\n",
        "labels = np.array(train_dataset.targets)\n",
        "labeled_inx = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBtVj1MY-nmY",
        "outputId": "c66e93df-842a-4254-dacf-5faaa392a833"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 336kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.17MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.10MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Step 1: Data Preparation\n",
        "# -------------------------\n",
        "\n",
        "# Transform: Normalize MNIST images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load full MNIST dataset\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "# Create labeled subset (e.g., 100 examples evenly distributed across classes)\n",
        "num_labels = 100\n",
        "labels = np.array(train_dataset.targets)\n",
        "labeled_idx = []\n",
        "\n",
        "for i in range(10):\n",
        "    idx = np.where(labels == i)[0][:num_labels // 10]\n",
        "    labeled_idx.extend(idx)\n",
        "\n",
        "unlabeled_idx = list(set(range(len(train_dataset))) - set(labeled_idx))\n",
        "\n",
        "labeled_dataset = Subset(train_dataset, labeled_idx)\n",
        "unlabeled_dataset = Subset(train_dataset, unlabeled_idx)\n",
        "\n",
        "labeled_loader = DataLoader(labeled_dataset, batch_size=64, shuffle=True)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "0Dr1bCT4AuAb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add noise\n",
        "\n",
        "class GaussianNoise(nn.Module):\n",
        "    def __init__(self, stddev):\n",
        "        super().__init__()\n",
        "        self.stddev = stddev\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            noise = torch.randn_like(x) * self.stddev\n",
        "            return x + noise\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, noise_std):\n",
        "        super().__init__()\n",
        "        self.noise = GaussianNoise(noise_std)\n",
        "        self.fc1 = nn.Linear(784, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 500)\n",
        "        self.fc3 = nn.Linear(500, 250)\n",
        "        self.fc4 = nn.Linear(250, 250)\n",
        "        self.fc5 = nn.Linear(250, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = []\n",
        "        x = x.view(-1, 784)\n",
        "        x = self.noise(x)\n",
        "        z1 = self.fc1(x)\n",
        "        z.append(z1)\n",
        "        z2 = self.fc2(F.relu(z1))\n",
        "        z.append(z2)\n",
        "        z3 = self.fc3(F.relu(z2))\n",
        "        z.append(z3)\n",
        "        z4 = self.fc4(F.relu(z3))\n",
        "        z.append(z4)\n",
        "        z5 = self.fc5(F.relu(z4))\n",
        "        z.append(z5)\n",
        "        return z\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(10, 250)    # reconstruct z4 (250)\n",
        "        self.fc2 = nn.Linear(250, 250)   # reconstruct z3 (250)\n",
        "        self.fc3 = nn.Linear(250, 500)   # reconstruct z2 (500)\n",
        "        self.fc4 = nn.Linear(500, 1000)  # reconstruct z1 (1000)\n",
        "\n",
        "    def forward(self, z_corr):\n",
        "        d1 = self.fc1(z_corr[-1])              # input: z5 (10)\n",
        "        d2 = self.fc2(F.relu(d1))              # matches z3\n",
        "        d3 = self.fc3(F.relu(d2))              # matches z2\n",
        "        d4 = self.fc4(F.relu(d3))              # matches z1\n",
        "        return [d4, d3, d2, d1]  # decoder outputs for z1 to z4"
      ],
      "metadata": {
        "id": "4bNbD0BuAw3n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Step 3: Training Functions\n",
        "# -------------------------\n",
        "\n",
        "def supervised_loss(output, target):\n",
        "    return F.cross_entropy(output, target)\n",
        "\n",
        "def reconstruction_loss(z_clean, z_recon):\n",
        "    loss = 0\n",
        "    # Match z1 to z4 with d4 to d1\n",
        "    for zc, zr in zip(z_clean[:4], z_recon):  # z_clean[:4] = z1 to z4\n",
        "        loss += F.mse_loss(zr, zc.detach())\n",
        "    return loss\n",
        "\n",
        "def train_epoch(encoder, decoder, optimizer, labeled_loader, unlabeled_loader, alpha):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    for (x_l, y_l), (x_u, _) in zip(labeled_loader, unlabeled_loader):\n",
        "        x_l, y_l = x_l.to(device), y_l.to(device)\n",
        "        x_u = x_u.to(device)\n",
        "\n",
        "        z_corr_l = encoder(x_l)\n",
        "        z_clean_l = encoder(x_l)\n",
        "        z_corr_u = encoder(x_u)\n",
        "        z_clean_u = encoder(x_u)\n",
        "\n",
        "        output = z_corr_l[-1]\n",
        "        loss_sup = supervised_loss(output, y_l)\n",
        "\n",
        "        recon_l = decoder(z_corr_l)\n",
        "        recon_u = decoder(z_corr_u)\n",
        "\n",
        "        loss_unsup = reconstruction_loss(z_clean_l, recon_l) + reconstruction_loss(z_clean_u, recon_u)\n",
        "\n",
        "        loss = loss_sup + alpha * loss_unsup\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate(encoder, loader):\n",
        "    encoder.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            output = encoder(x)[-1]\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "    return correct / len(loader.dataset)\n"
      ],
      "metadata": {
        "id": "NdULHBEkAy-i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Step 4: Training Loop\n",
        "# -------------------------\n",
        "\n",
        "encoder = Encoder(noise_std=0.3).to(device)\n",
        "decoder = Decoder().to(device)\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)\n",
        "\n",
        "epochs = 30\n",
        "alpha = 0.5\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_epoch(encoder, decoder, optimizer, labeled_loader, unlabeled_loader, alpha)\n",
        "    test_acc = evaluate(encoder, test_loader)\n",
        "    labeled_acc = evaluate(encoder, labeled_loader)\n",
        "    unlabeled_acc = evaluate(encoder, DataLoader(unlabeled_dataset, batch_size=256))\n",
        "    print(f\"Epoch {epoch:02d} | Test Acc: {test_acc:.4f} | Labeled Acc: {labeled_acc:.4f} | Unlabeled Pseudo Acc: {unlabeled_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FkXwywiA1PF",
        "outputId": "49154da1-cb1b-4456-f5ba-42eb069534d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Test Acc: 0.5021 | Labeled Acc: 0.6900 | Unlabeled Pseudo Acc: 0.5004\n",
            "Epoch 02 | Test Acc: 0.5071 | Labeled Acc: 0.7600 | Unlabeled Pseudo Acc: 0.5126\n",
            "Epoch 03 | Test Acc: 0.5318 | Labeled Acc: 0.7500 | Unlabeled Pseudo Acc: 0.5344\n",
            "Epoch 04 | Test Acc: 0.4756 | Labeled Acc: 0.7500 | Unlabeled Pseudo Acc: 0.4818\n",
            "Epoch 05 | Test Acc: 0.3654 | Labeled Acc: 0.6900 | Unlabeled Pseudo Acc: 0.3757\n",
            "Epoch 06 | Test Acc: 0.6824 | Labeled Acc: 0.9100 | Unlabeled Pseudo Acc: 0.6873\n",
            "Epoch 07 | Test Acc: 0.5987 | Labeled Acc: 0.9600 | Unlabeled Pseudo Acc: 0.6081\n",
            "Epoch 08 | Test Acc: 0.5500 | Labeled Acc: 0.9400 | Unlabeled Pseudo Acc: 0.5630\n",
            "Epoch 09 | Test Acc: 0.6050 | Labeled Acc: 0.9600 | Unlabeled Pseudo Acc: 0.6134\n",
            "Epoch 10 | Test Acc: 0.6733 | Labeled Acc: 0.9700 | Unlabeled Pseudo Acc: 0.6777\n",
            "Epoch 11 | Test Acc: 0.6852 | Labeled Acc: 0.9900 | Unlabeled Pseudo Acc: 0.6895\n",
            "Epoch 12 | Test Acc: 0.6938 | Labeled Acc: 0.9900 | Unlabeled Pseudo Acc: 0.6941\n",
            "Epoch 13 | Test Acc: 0.7092 | Labeled Acc: 0.9900 | Unlabeled Pseudo Acc: 0.7104\n",
            "Epoch 14 | Test Acc: 0.7087 | Labeled Acc: 0.9900 | Unlabeled Pseudo Acc: 0.7094\n",
            "Epoch 15 | Test Acc: 0.6910 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.6940\n",
            "Epoch 16 | Test Acc: 0.6832 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.6870\n",
            "Epoch 17 | Test Acc: 0.7038 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.7050\n",
            "Epoch 18 | Test Acc: 0.7101 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.7105\n",
            "Epoch 19 | Test Acc: 0.6835 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.6879\n",
            "Epoch 20 | Test Acc: 0.6699 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.6752\n",
            "Epoch 21 | Test Acc: 0.6783 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.6851\n",
            "Epoch 22 | Test Acc: 0.6638 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.6713\n",
            "Epoch 23 | Test Acc: 0.6614 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.6699\n",
            "Epoch 24 | Test Acc: 0.6570 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.6667\n",
            "Epoch 25 | Test Acc: 0.6336 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.6466\n",
            "Epoch 26 | Test Acc: 0.6411 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.6519\n",
            "Epoch 27 | Test Acc: 0.6302 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.6397\n",
            "Epoch 28 | Test Acc: 0.6548 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.6633\n",
            "Epoch 29 | Test Acc: 0.6352 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.6468\n",
            "Epoch 30 | Test Acc: 0.6213 | Labeled Acc: 1.0000 | Unlabeled Pseudo Acc: 0.6324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIE Models"
      ],
      "metadata": {
        "id": "Q_JvJI2gBwxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PI - Network\n",
        "\n",
        "# -------------------------\n",
        "# Step 2: Define the Model\n",
        "# -------------------------\n",
        "\n",
        "class PiCNN(nn.Module):\n",
        "    \"\"\"Simple CNN for Π Model\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))        # Conv layer 1\n",
        "        x = F.relu(self.conv2(x))        # Conv layer 2\n",
        "        x = F.max_pool2d(x, 2)           # Max pooling\n",
        "        x = torch.flatten(x, 1)          # Flatten to [batch, features]\n",
        "        x = self.dropout(x)              # Apply dropout\n",
        "        x = F.relu(self.fc1(x))          # Fully connected\n",
        "        x = self.fc2(x)                  # Output logits\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Tra66Q3LB0IP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Step 3: Training Utilities\n",
        "# -------------------------\n",
        "\n",
        "def add_noise(x, noise_std=0.15):\n",
        "    \"\"\"Adds Gaussian noise to an input tensor (augmentation)\"\"\"\n",
        "    return x + torch.randn_like(x) * noise_std\n",
        "\n",
        "def train_pi_model(model, optimizer, labeled_loader, unlabeled_loader, alpha):\n",
        "    model.train()\n",
        "    for (x_l, y_l), (x_u, _) in zip(labeled_loader, unlabeled_loader):\n",
        "        x_l, y_l = x_l.to(device), y_l.to(device)\n",
        "        x_u = x_u.to(device)\n",
        "\n",
        "        # Supervised loss on labeled data (with dropout)\n",
        "        logits_l = model(x_l)\n",
        "        loss_sup = F.cross_entropy(logits_l, y_l)\n",
        "\n",
        "        # Consistency loss on unlabeled data\n",
        "        # Pass same input twice with different dropout masks and augmentations\n",
        "        x_u1 = add_noise(x_u)\n",
        "        x_u2 = add_noise(x_u)\n",
        "\n",
        "        logits_u1 = model(x_u1)\n",
        "        logits_u2 = model(x_u2)\n",
        "\n",
        "        probs_u1 = F.softmax(logits_u1, dim=1)\n",
        "        probs_u2 = F.softmax(logits_u2, dim=1)\n",
        "\n",
        "        loss_unsup = F.mse_loss(probs_u1, probs_u2)\n",
        "\n",
        "        # Total loss\n",
        "        loss = loss_sup + alpha * loss_unsup\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            output = model(x)\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "    return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "Ecv_u6AAB8p8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Step 4: Train the Model\n",
        "# -------------------------\n",
        "\n",
        "model = PiCNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 20\n",
        "alpha = 20.0  # weight for consistency loss\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_pi_model(model, optimizer, labeled_loader, unlabeled_loader, alpha)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f\"Epoch {epoch:02d} | Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waJt4Y_UCAlV",
        "outputId": "96013e7a-efa4-4906-8498-c88168287412"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Test Accuracy: 0.2972\n",
            "Epoch 02 | Test Accuracy: 0.5309\n",
            "Epoch 03 | Test Accuracy: 0.6740\n",
            "Epoch 04 | Test Accuracy: 0.6927\n",
            "Epoch 05 | Test Accuracy: 0.7014\n",
            "Epoch 06 | Test Accuracy: 0.6991\n",
            "Epoch 07 | Test Accuracy: 0.6940\n",
            "Epoch 08 | Test Accuracy: 0.6979\n",
            "Epoch 09 | Test Accuracy: 0.7095\n",
            "Epoch 10 | Test Accuracy: 0.7174\n",
            "Epoch 11 | Test Accuracy: 0.7184\n",
            "Epoch 12 | Test Accuracy: 0.7299\n",
            "Epoch 13 | Test Accuracy: 0.7232\n",
            "Epoch 14 | Test Accuracy: 0.6992\n",
            "Epoch 15 | Test Accuracy: 0.7063\n",
            "Epoch 16 | Test Accuracy: 0.7198\n",
            "Epoch 17 | Test Accuracy: 0.7080\n",
            "Epoch 18 | Test Accuracy: 0.6979\n",
            "Epoch 19 | Test Accuracy: 0.7023\n",
            "Epoch 20 | Test Accuracy: 0.7178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self Training"
      ],
      "metadata": {
        "id": "OLe_ZndjCCG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# self training\n",
        "\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset, Dataset\n",
        "\n",
        "# Step 2: CNN for self-training\n",
        "class BaseCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Step 3: Supervised training\n",
        "def train(model, loader, optimizer):\n",
        "    model.train()\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = F.cross_entropy(model(x), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Step 4: Generate pseudo-labels for unlabeled data\n",
        "def generate_pseudo_labels(model, loader, threshold=0.8):\n",
        "    model.eval()\n",
        "    pseudo_x, pseudo_y = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            logits = model(x)\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            conf, pred = torch.max(probs, 1)\n",
        "            mask = conf > threshold\n",
        "            if mask.any():\n",
        "                pseudo_x.append(x[mask])\n",
        "                pseudo_y.append(pred[mask])\n",
        "    if pseudo_x:\n",
        "        return torch.utils.data.TensorDataset(torch.cat(pseudo_x), torch.cat(pseudo_y))\n",
        "    return None\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "# Step 5: Self-Training Loop\n",
        "model = BaseCNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    train(model, labeled_loader, optimizer)\n",
        "    pseudo_dataset = generate_pseudo_labels(model, unlabeled_loader, threshold=0.95)\n",
        "    if pseudo_dataset:\n",
        "        print(f\"Epoch {epoch}: Adding {len(pseudo_dataset)} pseudo-labeled samples.\")\n",
        "        labeled_set = ConcatDataset([labeled_dataset, pseudo_dataset])\n",
        "        labeled_loader = DataLoader(labeled_dataset, batch_size=64, shuffle=True)\n",
        "    acc = evaluate(model, test_loader)\n",
        "    print(f\"[Self-Training] Epoch {epoch} - Test Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-pr4t04CD-r",
        "outputId": "0575a433-5569-4a77-f62d-a1649685a10d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Self-Training] Epoch 1 - Test Accuracy: 0.2255\n",
            "[Self-Training] Epoch 2 - Test Accuracy: 0.3789\n",
            "[Self-Training] Epoch 3 - Test Accuracy: 0.5263\n",
            "[Self-Training] Epoch 4 - Test Accuracy: 0.6021\n",
            "[Self-Training] Epoch 5 - Test Accuracy: 0.6258\n",
            "[Self-Training] Epoch 6 - Test Accuracy: 0.6473\n",
            "Epoch 7: Adding 1 pseudo-labeled samples.\n",
            "[Self-Training] Epoch 7 - Test Accuracy: 0.6713\n",
            "Epoch 8: Adding 215 pseudo-labeled samples.\n",
            "[Self-Training] Epoch 8 - Test Accuracy: 0.6696\n",
            "Epoch 9: Adding 1348 pseudo-labeled samples.\n",
            "[Self-Training] Epoch 9 - Test Accuracy: 0.6701\n",
            "Epoch 10: Adding 4459 pseudo-labeled samples.\n",
            "[Self-Training] Epoch 10 - Test Accuracy: 0.6713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Co-Trainig"
      ],
      "metadata": {
        "id": "Av10OJ3rCIlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Co - Training\n",
        "class SplitMNIST(Dataset):\n",
        "    def __init__(self, dataset, side='left'):\n",
        "        self.dataset = dataset\n",
        "        self.side = side\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.dataset[idx]\n",
        "        if self.side == 'left':\n",
        "            return x[:, :, :14], y\n",
        "        else:\n",
        "            return x[:, :, 14:], y\n",
        "\n",
        "# Step 3: Define Half-CNN Model\n",
        "class HalfCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 16, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(16 * 7 * 3, 64)  # Actually matches input from 28x14 images\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))     # → [B, 16, 14, 7]\n",
        "        x = self.pool(F.relu(self.conv2(x)))     # → [B, 16, 7, 3]\n",
        "        x = x.view(x.size(0), -1)                # Flatten safely\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Step 4: Training and Evaluation Functions\n",
        "def train(model, loader, optimizer):\n",
        "    model.train()\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = F.cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "def generate_pseudo_labels(model, loader, threshold=0.5):\n",
        "    model.eval()\n",
        "    pseudo_x, pseudo_y = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            logits = model(x)\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            conf, pred = torch.max(probs, 1)\n",
        "            mask = conf > threshold\n",
        "            if mask.any():\n",
        "                pseudo_x.append(x[mask].cpu())\n",
        "                pseudo_y.append(pred[mask].cpu())\n",
        "    if pseudo_x:\n",
        "        return torch.utils.data.TensorDataset(torch.cat(pseudo_x), torch.cat(pseudo_y))\n",
        "    return None\n",
        "\n",
        "# Step 5: Initialize Models and Loaders\n",
        "model1 = HalfCNN().to(device)\n",
        "model2 = HalfCNN().to(device)\n",
        "opt1 = optim.Adam(model1.parameters(), lr=1e-3)\n",
        "opt2 = optim.Adam(model2.parameters(), lr=1e-3)\n",
        "\n",
        "view1_loader = DataLoader(SplitMNIST(labeled_dataset, 'left'), batch_size=64, shuffle=True)\n",
        "view2_loader = DataLoader(SplitMNIST(labeled_dataset, 'right'), batch_size=64, shuffle=True)\n",
        "unlabeled1 = DataLoader(SplitMNIST(unlabeled_dataset, 'left'), batch_size=256)\n",
        "unlabeled2 = DataLoader(SplitMNIST(unlabeled_dataset, 'right'), batch_size=256)\n",
        "\n",
        "# Step 6: Co-Training Loop\n",
        "for epoch in range(1, 11):\n",
        "    print(f\"\\nEpoch {epoch}\")\n",
        "\n",
        "    # Train both models on current labeled data\n",
        "    train(model1, view1_loader, opt1)\n",
        "    train(model2, view2_loader, opt2)\n",
        "\n",
        "    # Generate pseudo-labels\n",
        "    p1 = generate_pseudo_labels(model1, unlabeled1)\n",
        "    p2 = generate_pseudo_labels(model2, unlabeled2)\n",
        "\n",
        "    if p1 and p2:\n",
        "        print(f\"  Adding pseudo-labels: View1 ← {len(p2)} from model2, View2 ← {len(p1)} from model1\")\n",
        "        pseudo_view1 = SplitMNIST(p1, side='right')   # model1 gets help from model2\n",
        "        pseudo_view2 = SplitMNIST(p2, side='left')    # model2 gets help from model1\n",
        "\n",
        "        view1_loader = DataLoader(ConcatDataset([SplitMNIST(labeled_set, 'left'), pseudo_view2]), batch_size=64, shuffle=True)\n",
        "        view2_loader = DataLoader(ConcatDataset([SplitMNIST(labeled_set, 'right'), pseudo_view1]), batch_size=64, shuffle=True)\n",
        "\n",
        "    acc1 = evaluate(model1, unlabeled1)\n",
        "    acc2 = evaluate(model2, unlabeled2)\n",
        "    print(f\"  View1 Accuracy: {acc1:.4f}, View2 Accuracy: {acc2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je1qHokxCKbZ",
        "outputId": "91005df4-eed4-4a99-cfd7-914c01259233"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "  View1 Accuracy: 0.2056, View2 Accuracy: 0.1029\n",
            "\n",
            "Epoch 2\n",
            "  View1 Accuracy: 0.2397, View2 Accuracy: 0.1191\n",
            "\n",
            "Epoch 3\n",
            "  View1 Accuracy: 0.3042, View2 Accuracy: 0.1711\n",
            "\n",
            "Epoch 4\n",
            "  View1 Accuracy: 0.3495, View2 Accuracy: 0.2230\n",
            "\n",
            "Epoch 5\n",
            "  View1 Accuracy: 0.4269, View2 Accuracy: 0.2686\n",
            "\n",
            "Epoch 6\n",
            "  View1 Accuracy: 0.4618, View2 Accuracy: 0.3609\n",
            "\n",
            "Epoch 7\n",
            "  View1 Accuracy: 0.5020, View2 Accuracy: 0.4410\n",
            "\n",
            "Epoch 8\n",
            "  View1 Accuracy: 0.5256, View2 Accuracy: 0.4662\n",
            "\n",
            "Epoch 9\n",
            "  View1 Accuracy: 0.5412, View2 Accuracy: 0.4808\n",
            "\n",
            "Epoch 10\n",
            "  View1 Accuracy: 0.5532, View2 Accuracy: 0.4937\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}